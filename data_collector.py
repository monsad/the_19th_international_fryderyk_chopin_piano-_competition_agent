
import httpx
import feedparser
from bs4 import BeautifulSoup
from typing import List, Optional, Dict
from datetime import datetime, timedelta
from models import PerformanceData, SourceType, CompetitionStage
from config import settings
import asyncio
import hashlib
import json
import re

class CompetitionWebsiteCollector:
    """Collector for official Chopin Competition website"""
    
    async def collect(self, days_back: int = 30) -> List[PerformanceData]:
        performances = []
        cutoff_date = datetime.now() - timedelta(days=days_back)
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            for url in settings.COMPETITION_SOURCES:
                try:
                    performances.extend(await self._scrape_website(client, url, cutoff_date))
                except Exception as e:
                    print(f"Error collecting from {url}: {e}")
        
        return performances
    
    async def _scrape_website(self, client: httpx.AsyncClient, url: str, cutoff_date: datetime) -> List[PerformanceData]:
        try:
            print(f"üåê Scraping website: {url}")
            response = await client.get(url, follow_redirects=True)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            performances = []
            
            if 'chopincompetition.pl' in url:
                performances.extend(await self._scrape_chopincompetition_pl(client, soup, url))
            
            performance_sections = soup.find_all(['div', 'article'], class_=lambda x: x and ('performance' in str(x).lower() or 'pianist' in str(x).lower()))
            
            for section in performance_sections[:50]:
                try:
                    name_elem = section.find(['h2', 'h3', 'span'], class_=lambda x: x and 'name' in str(x).lower())
                    if not name_elem:
                        continue
                    
                    pianist_name = name_elem.get_text(strip=True)
                    
                    pieces = self._extract_pieces(section.get_text())
                    
                    stage_text = section.get_text().lower()
                    stage = self._determine_stage(stage_text)
                    
                    nationality = self._extract_nationality(section.get_text())
                    
                    performance = PerformanceData(
                        id=hashlib.md5((pianist_name + str(datetime.now())).encode()).hexdigest(),
                        pianist_name=pianist_name,
                        nationality=nationality or "Unknown",
                        stage=stage,
                        performance_date=datetime.now(),
                        pieces_performed=pieces,
                        video_url=self._extract_video_url(section),
                        source=url,
                        source_type=SourceType.COMPETITION_WEBSITE,
                        timestamp=datetime.now()
                    )
                    
                    performances.append(performance)
                    
                except Exception as e:
                    print(f"Error parsing section: {e}")
                    continue
            
            return performances
            
        except Exception as e:
            print(f"Error scraping {url}: {e}")
            return []
    
    def _extract_pieces(self, text: str) -> List[str]:
        """Extract Chopin pieces from text"""
        pieces = []
        
        # Szukaj wzorc√≥w jak "Op. 10", "Ballade No. 1", etc.
        patterns = [
            r'Op\.\s*\d+',
            r'Ballade No\.\s*\d+',
            r'Scherzo No\.\s*\d+',
            r'Etude.*?Op\.\s*\d+',
            r'Polonaise.*?Op\.\s*\d+',
            r'Concerto No\.\s*\d+',
            r'Sonata No\.\s*\d+'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            pieces.extend(matches)
        
        return list(set(pieces))[:10]  # Max 10, unique
    
    def _determine_stage(self, text: str) -> CompetitionStage:
        """Determine competition stage from text"""
        text_lower = text.lower()
        
        if 'final' in text_lower:
            return CompetitionStage.FINAL
        elif 'stage 3' in text_lower or 'third stage' in text_lower:
            return CompetitionStage.STAGE_3
        elif 'stage 2' in text_lower or 'second stage' in text_lower:
            return CompetitionStage.STAGE_2
        elif 'stage 1' in text_lower or 'first stage' in text_lower:
            return CompetitionStage.STAGE_1
        else:
            return CompetitionStage.PRELIMINARY
    
    def _extract_nationality(self, text: str) -> Optional[str]:
        """Extract nationality from text"""
        # Lista kraj√≥w uczestniczƒÖcych w Konkursie Chopinowskim
        countries = [
            "Poland", "China", "Japan", "Russia", "South Korea", 
            "United States", "Canada", "France", "Germany", "Italy",
            "Ukraine", "Spain", "United Kingdom", "Taiwan", "Vietnam"
        ]
        
        for country in countries:
            if country.lower() in text.lower():
                return country
        
        return None
    
    def _extract_video_url(self, section) -> Optional[str]:
        """Extract video URL if present"""
        video_link = section.find('a', href=lambda x: x and ('youtube' in str(x).lower() or 'video' in str(x).lower()))
        if video_link:
            return video_link.get('href')
        return None
    
    async def _scrape_chopincompetition_pl(self, client: httpx.AsyncClient, soup, base_url: str) -> List[PerformanceData]:
        """Specjalna obs≈Çuga dla oficjalnej strony chopincompetition.pl"""
        performances = []
        
        try:
            print("   üéπ Parsowanie chopincompetition.pl...")
            
            # Szukaj link√≥w do uczestnik√≥w (competitors)
            competitors_link = soup.find('a', href=lambda x: x and 'competitors' in str(x).lower())
            if competitors_link and competitors_link.get('href'):
                competitor_url = competitors_link['href']
                if not competitor_url.startswith('http'):
                    competitor_url = 'https://www.chopincompetition.pl' + competitor_url
                
                print(f"      üìã Pobieranie listy uczestnik√≥w z: {competitor_url}")
                
                # Pobierz stronƒô z uczestnikami
                try:
                    comp_response = await client.get(competitor_url, follow_redirects=True)
                    comp_soup = BeautifulSoup(comp_response.text, 'html.parser')
                    
                    # Szukaj nazwisk pianist√≥w - r√≥≈ºne mo≈ºliwe struktury
                    pianist_names = set()
                    
                    # Metoda 1: Szukaj w tek≈õcie wzorc√≥w nazwisk
                    all_text = comp_soup.get_text()
                    # Wz√≥r: Imiƒô Nazwisko (z du≈ºych liter na poczƒÖtku)
                    name_matches = re.findall(r'\b([A-Z][a-z]+\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\b', all_text)
                    
                    # Filtruj typowe s≈Çowa kt√≥re nie sƒÖ nazwiskami
                    exclude_words = {
                        'Chopin Competition', 'The Chopin', 'Piano Competition', 'First Stage', 
                        'Second Stage', 'Third Stage', 'Competition Stage', 'View All', 'More Info',
                        'Read More', 'Find Out', 'Click Here', 'Follow Us', 'About Us',
                        'Competition International', 'International Fryderyk', 'Fryderyk Chopin',
                        'Chopin Piano', 'Piano Chopin', 'Konkurs Pianistyczny', 'Pianistyczny Im',
                        'Im Fryderyka', 'Special Inaugural', 'Inaugural Concert', 'Morning Session',
                        'Evening Session', 'First Round', 'Second Round', 'Third Round'
                    }
                    
                    for name in name_matches:
                        is_excluded = any(excl.lower() in name.lower() for excl in exclude_words)
                        
                        keywords_to_skip = ['competition', 'chopin', 'fryderyk', 'konkurs', 'pianistyczny', 'international']
                        has_keyword = any(kw in name.lower() for kw in keywords_to_skip)
                        
                        if not is_excluded and not has_keyword and len(name.split()) >= 2:
                            pianist_names.add(name)
                    
                    print(f"      ‚úÖ Znaleziono {len(pianist_names)} pianist√≥w na stronie")
                    
                    for name in list(pianist_names)[:50]:  # Max 50 ≈ºeby nie przeciƒÖ≈ºyƒá
                        performance = PerformanceData(
                            id=hashlib.md5((name + "competitors").encode()).hexdigest(),
                            pianist_name=name,
                            nationality="Unknown",
                            stage=CompetitionStage.STAGE_1,
                            performance_date=datetime.now(),
                            pieces_performed=[],
                            video_url=None,
                            source=competitor_url,
                            source_type=SourceType.COMPETITION_WEBSITE,
                            timestamp=datetime.now()
                        )
                        performances.append(performance)
                
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  B≈ÇƒÖd pobierania uczestnik√≥w: {e}")
            
            news_items = soup.find_all(['article', 'div'], class_=lambda x: x and 'news' in str(x).lower())
            print(f"      üì∞ Znaleziono {len(news_items)} element√≥w news√≥w")
            
            for item in news_items[:20]:
                try:
                    title = item.find(['h1', 'h2', 'h3', 'h4'])
                    if not title:
                        continue
                    
                    title_text = title.get_text(strip=True)
                    
                    pianist_match = re.search(r'\b([A-Z][a-z]+\s+[A-Z][a-z]+)\b', title_text)
                    if pianist_match:
                        pianist_name = pianist_match.group(1)
                        
                        description = item.get_text()[:500]
                        pieces = self._extract_pieces(description)
                        stage = self._determine_stage(description)
                        
                        performance = PerformanceData(
                            id=hashlib.md5((pianist_name + base_url).encode()).hexdigest(),
                            pianist_name=pianist_name,
                            nationality=self._extract_nationality(description) or "Unknown",
                            stage=stage,
                            performance_date=datetime.now(),
                            pieces_performed=pieces,
                            video_url=None,
                            source=base_url,
                            source_type=SourceType.COMPETITION_WEBSITE,
                            timestamp=datetime.now()
                        )
                        performances.append(performance)
                        print(f"Znaleziono pianistƒô: {pianist_name}")
                
                except Exception as e:
                    continue
            
        except Exception as e:
            print(f"B≈ÇƒÖd parsowania chopincompetition.pl: {e}")
        
        return performances


class YouTubeCollector:
    """Collector for YouTube performances and reviews"""
    
    def __init__(self):
        self.api_key = settings.YOUTUBE_API_KEY
    
    async def collect(self, search_query: str = "Chopin Competition 2025", max_results: int = 50) -> List[PerformanceData]:
        if not self.api_key:
            print("YouTube API key not provided, skipping YouTube collection")
            return []
        
        performances = []
        
        try:
            from googleapiclient.discovery import build
            
            youtube = build('youtube', 'v3', developerKey=self.api_key)
            
            # Wyszukaj wideo
            search_response = youtube.search().list(
                q=search_query,
                part='snippet',
                maxResults=max_results,
                type='video',
                order='relevance'
            ).execute()
            
            items = search_response.get('items', [])
            print(f"üì∫ Found {len(items)} YouTube videos for '{search_query}'")
            
            for item in items:
                try:
                    video_id = item['id']['videoId']
                    snippet = item['snippet']
                    
                    # WyciƒÖgnij nazwisko pianisty z tytu≈Çu
                    title = snippet['title']
                    print(f"Video: {title[:80]}...")
                    pianist_name = self._extract_pianist_name(title)
                    
                    if not pianist_name:
                        print(f"Could not extract pianist name, skipping")
                        continue
                    
                    print(f"Found pianist: {pianist_name}")
                    
                    # WyciƒÖgnij utwory z opisu
                    description = snippet['description']
                    pieces = self._extract_pieces_from_description(description)
                    
                    performance = PerformanceData(
                        id=video_id,
                        pianist_name=pianist_name,
                        nationality="Unknown",
                        stage=CompetitionStage.STAGE_1,  # Domy≈õlnie
                        performance_date=datetime.fromisoformat(snippet['publishedAt'].replace('Z', '+00:00')),
                        pieces_performed=pieces,
                        video_url=f"https://www.youtube.com/watch?v={video_id}",
                        source="YouTube",
                        source_type=SourceType.YOUTUBE,
                        timestamp=datetime.now()
                    )
                    
                    performances.append(performance)
                    
                except Exception as e:
                    print(f"Error parsing YouTube video: {e}")
                    continue
            
        except Exception as e:
            print(f"Error collecting from YouTube: {e}")
        
        return performances
    
    def _extract_pianist_name(self, title: str) -> Optional[str]:
        """Extract pianist name from video title"""
        # Wzorce do wyciƒÖgania nazwiska
        # Przyk≈Çad: "ERIC LU ‚Äì second round" lub "John Smith plays Chopin"
        
        patterns = [
            r'^([A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]+(?:\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]+)+)\s+[‚Äì-]',
            r'^([A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\s+plays',
            r'^([A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\s+-',
            r'Pianist:\s+([A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)',
            r'^([A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]+(?:\s+[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]+){1,3})\s+[‚Äì-‚Äì‚Äî]',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                name = match.group(1).strip()
                # Konwertuj na Title Case dla sp√≥jno≈õci
                return name.title()
        
        return None
    
    def _extract_pieces_from_description(self, description: str) -> List[str]:
        """Extract piece names from description"""
        pieces = []
        
        lines = description.split('\n')
        for line in lines:
            if 'op.' in line.lower() or 'ballade' in line.lower() or 'etude' in line.lower():
                pieces.append(line.strip()[:100])
        
        return pieces[:5]


class NewsReviewCollector:
    """Collector for news and expert reviews"""
    
    async def collect(self, days_back: int = 30) -> List[Dict]:
        reviews = []
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            for source_url in settings.NEWS_SOURCES:
                try:
                    reviews.extend(await self._scrape_news(client, source_url, days_back))
                except Exception as e:
                    print(f"Error collecting news from {source_url}: {e}")
        
        return reviews
    
    async def _scrape_news(self, client: httpx.AsyncClient, url: str, days_back: int) -> List[Dict]:
        try:
            response = await client.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            reviews = []
            articles = soup.find_all(['article', 'div'], class_=lambda x: x and ('article' in str(x).lower() or 'post' in str(x).lower()))
            
            for article in articles[:20]:
                title_elem = article.find(['h1', 'h2', 'h3'])
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                
                # Sprawd≈∫ czy dotyczy Chopina/konkursu
                if not self._is_chopin_related(title):
                    continue
                
                content = article.get_text(strip=True)[:1000]
                
                reviews.append({
                    "title": title,
                    "content": content,
                    "source": url,
                    "url": self._extract_article_url(article, url),
                    "timestamp": datetime.now(),
                    "pianist_mentioned": self._extract_mentioned_pianists(content)
                })
            
            return reviews
            
        except Exception as e:
            print(f"Error scraping news from {url}: {e}")
            return []
    
    def _is_chopin_related(self, text: str) -> bool:
        """Check if text is related to Chopin Competition"""
        keywords = ['chopin', 'competition', 'piano', 'pianist', 'warsaw', 'contest']
        text_lower = text.lower()
        return sum(1 for kw in keywords if kw in text_lower) >= 2
    
    def _extract_article_url(self, article, base_url: str) -> str:
        """Extract article URL"""
        link = article.find('a', href=True)
        if link:
            href = link['href']
            if href.startswith('http'):
                return href
            else:
                return base_url.rstrip('/') + '/' + href.lstrip('/')
        return base_url
    
    def _extract_mentioned_pianists(self, text: str) -> List[str]:
        """Extract pianist names mentioned in text"""
        # Prosty wzorzec: wielka litera + ma≈Çe + spacja + wielka litera + ma≈Çe
        pattern = r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b'
        matches = re.findall(pattern, text)
        
        # Filtruj typowe fa≈Çszywe dopasowania
        exclude = ['New York', 'Los Angeles', 'United States', 'The Guardian']
        
        return [m for m in matches if m not in exclude][:5]


class SocialMediaCollector:
    """Collector for social media sentiment (Twitter, forums)"""
    
    async def collect(self, search_terms: List[str] = None) -> List[Dict]:
        """
        Zbiera opinie z medi√≥w spo≈Çeczno≈õciowych
        W praktycznej implementacji po≈ÇƒÖczy≈Çby siƒô z Twitter API, Reddit API, etc.
        """
        # Placeholder - w pe≈Çnej wersji pod≈ÇƒÖcz Twitter API
        print("Social media collection - placeholder implementation")
        return []